تقرير بحثي شامل لتحسين محركات
البحث: استعادة وتعزيز موقع
aldeyarksa.tech
الأزمة التقنية الحاسمة: إصلاح ملف robots.txt
كأولوية قصوى
إن تحقيق الترتيب الأول في نتائج محركات البحث يبدأ بأساس تقني سليم، وهو القدرة الكاملة
لمحركات البحث على الوصول إلى محتوى الموقع وإدراك وجوده. بعد إجراء تحليل معمق للموقع
aldeyarksa.tech، تم تحديد مشكلة تقنية خطيرة ومباشرة تشكل عائقًا شبه كامل أمام أي
جهد مستقبلي في مجال تحسين محركات البحث (SEO). هذه المشكلة ليست مجرد خطأ فني
صغير يمكن إهماله، بل هي عبارة عن حصار تقني يعزل الموقع عن محركات البحث، مما يجعله
.غير مرئي تمامًا بالنسبة لها. إنها الأزمة التقنية الحاسمة التي تتطلب أولوية قصوى وإجراءً فورياً
المشكلة الجوهرية تكمن في ملف robots.txt، وهو ملف نصي بسيط يوُضع في الدليل الجذر
لاسم المجال (مثل http://www.aldeyarksa.tech/robots.txt) ويوجه محركات
تاريخياً، كان هذا الملف أداة لإدارة ميزانية التصفح . البحث حول كيفية تصفح صفحات الموقع
(crawl budget)، حيث يمكن للمواقع إعلام المحركات بأقسامها التي يجب تجاهلها. ومع ذلك، فإن
.استخدامه اليوم أصبح له آثار مباشرة على قابلية الفهرسة والظهور
الخطأ الأكثر إلحاحًا الموجود على موقع aldeyarksa.tech يتمثل في قاعدة أساسية موجودة
في ملف robots.txt وهي 'User-agent: *' متبوعة مباشرة ب 'Disallow: /' .
تشير هذه القاعدة إلى محركات البحث بأنها يجب أن تمنع جميع المتصفحات الآلية (المعرفة باسم
"bots" أو "crawlers")، بما في ذلك متصفح Googlebot الرئيسي، من زيارة أي صفحة ضمن نطاق
نتيجة لهذه القاعدة، فإن الصفحة الرئيسية نفسها، وكذلك جميع الصفحات الأخرى التي . الموقع
تحتوي على المشاريع، والعروض، والصفحات الثانوية، تكون محصورة بالكامل ولا يمكن لمحركات
البحث الوصول إليها أو قراءتها. هذا يعني أن محركات البحث قد تجد روابط داخلية أو خارجية
تشير إلى الموقع، لكنها ستتوقف عند الحائل الذي يفرضه ملف robots.txt وستعتبر كل شيء
مرفوضًا. هذا يخلق حالة من العزلة الرقمية؛ فالموقع موجود فعلياً على الإنترنت، ولكنه غير
موجود من وجهة نظر محركات البحث. هذا الوضع ليس مجرد مشكلة في تحسين محركات
.البحث، بل هو قضية أساسية تتعلق بالوجود الرقمي نفسه
الأثر المدمر لهذه المشكلة لا يقتصر على عدم القدرة على الفهرسة. إنه يتفاقم ليشمل مجموعة
واسعة من التداعيات السلبية التي تضرب في صميم استراتيجية SEO. أولا،ً حتى لو كانت مؤسسة
الديار العالمية تمتلك شبكة قوية من الروابط الخارجية (Backlinks) من مواقع أخرى ذات سلطة
عالية، فإن هذا العمل سيكون بلا جدوى. محركات البحث، وبشكل أساسي Google، تعتمد على
الروابط الخارجية لتحديد مدى أهمية وموثوقية موقع ما. ولكن إذا لم يكن بإمكان متصفح
Googlebot زيارة الصفحة التي تحتوي على الرابط الخارجي، فإنه لن يتمكن من "تمرير" قيمة هذا
الرابط إلى موقع aldeyarksa.tech. وبالتالي، يتم إلغاء قيمة هذه الروابط قبل حتى أن يتم
تقييمها. ثانياً، هناك فقدان هائل للبيانات الهامة التي يوفرها الموقع. تشير المواد المرفقة إلى أن
2 10
7
7
الموقع يستخدم بيانات منظمة (Structured Data) في صفحة المعرض (Portfolio Page) عبر هيكل
PageMap هذه البيانات الغنية، والتي تشمل . لتقديم معلومات غنية حول المشاريع الموثقة
العنوان، الوصف، الموقع الجغرافي، وتقييمات المشاريع، تسمح لمحركات البحث بفهم المحتوى
بشكل أعمق وعرضه في شكل نتائج موسعة (Rich Snippets) في نتائج البحث. ومع ذلك، عندما
يكون ملف robots.txt يمنع الوصول، فإن محركات البحث لا ترى هذه البيانات المنظمة على
،الإطلاق، وبالتالي لا تستطيع الاستفادة منها لتعزيز ظهور النتائج أو جذب المزيد من النقرات. ثالثاً
هذه المشكلة تؤثر سلباً على ثقة محركات البحث في الموقع. وجود قاعدة حظر واضحة وشاملة
قد يشير إلى وجود خلل في إدارة الموقع أو قصدية في إخفاء المحتوى، مما قد يثير الشكوك لدى
خوارزميات Google حول موثوقية الموقع ككل. هذا يمكن أن يؤدي إلى تصنيف الموقع على أنه
.أقل موثوقية، مما يؤثر سلباً على مصداقيته في المستقبل
لذلك، فإن الحل المباشر والمباشر لهذه المشكلة هو التعديل الفوري على ملف robots.txt.
يجب تعديل القاعدة 'Disallow: /' لتنقل رسالة واضحة إلى محركات البحث بأنها مدعوة
لزيارة جميع أنحاء الموقع. الطريقة الأكثر شيوعًا وفعالية لهذا هي إضافة قاعدة Allow .محددة
على سبيل المثال، يمكن استبدال القاعدة الحالية بسلسلة من التعليمات التي تسمح بالوصول
إلى جميع المسارات. يمكن القيام بذلك باستخدام User-agent: Googlebot متبوعة ب
Allow: / لمنح الصلاحية حصرياً لمحرك البحث من Google . أو، بشكل أكثر عمومية، يمكن
استخدام User-agent: * مع Allow: / هذه . لمنح الصلاحية لجميع محركات البحث
التغييرات البسيطة ستقوم بإلغاء الحظر العام وستسمح لمتصفح Googlebot وغيره من الزوار
الآليين بالوصول إلى كافة صفحات الموقع. من الأهمية بمكان، كما يوصي الخبراء، قبل تطبيق
هذه التغييرات على البيئة الإنتاجية، يجب اختبارها باستخدام الأدوات المتاحة. تقدم Google Search
Console أداة "اختبار robots.txt" (Robots.txt Tester) التي تسمح لك بإدخال URL محدد واختبار
ما إذا كان سيتم حظره بواسطة القواعد الموجودة في ملف robots.txt هذه . الخاص بك
الأداة مفيدة للغاية لأنها توضح لك القاعدة الدقيقة التي تقوم بحظر الصفحة، مما يساعد على
تجنب الأخطاء المحتملة. بعد إجراء التعديل، يجب إعادة تحميل ملف robots.txt ،على الخادم
ثم استخدام Google Search Console لإعلام Google بالتغيير. يمكن القيام بذلك عن طريق طلب
إعادة تصفح الصفحة المحددة أو إعادة تقديم ملف XML sitemap، والذي سيحفز Googlebot على
. إعادة زيارتها
بالإضافة إلى هذه القاعدة الأساسية، من الضروري التحقق من وجود مشكلات robots.txt
الشائعة الأخرى التي قد تؤثر على الموقع. على سبيل المثال، من الممكن أن يكون ملف
robots.txt قد تم وضعه في مسار خاطئ، مثل دليل فرعي بدلا من الدليل الجذر، مما يجعله
كما يجب التأكد من أن الملف لا يحتوي على . غير قابل للاكتشاف من قبل محركات البحث
،أخطاء نحوية، مثل استخدام أحرف كبيرة وأصغر بشكل غير صحيح (على سبيل المثال
disallow بدلا من Disallow) أو وجود مسافات زائدة، حيث أن بعض إرشادات
robots.txt كما يجب تجنب استخدام إرشادات ميتة مثل . حساسة لحالة الأحرف
'noindex'، حيث أوقفت Google يجب أيضًا التأكد من . دعمها لهذه الإرشادة في سبتمبر 2019
أن ملف robots.txt لا يقوم بحظر الموارد الأساسية مثل ملفات CSS و JavaScript، لأن هذا
يمكن أن يعيق قدرة Googlebot من الممارسة . على فهم تصميم الصفحة ومحتواها بشكل صحيح
الجيدة أيضًا إضافة سطر Sitemap: في نهاية ملف robots.txt يشير إلى مسار ملف XML
sitemap هذا يسهل على محركات البحث اكتشاف ملف الهبوط الخاص بك . الخاص بالموقع
بشكل أسرع، خاصة وأن Googlebot يتحقق من ملف robots.txt في وقت مبكر جدًا من
على الرغم من أن إضافة ملف الهبوط ليس إلزامياً، إلا أنه يدعم أداء . عملية التصفح SEO بشكل
أفضل. بعد إجراء التعديلات، من الضروري مراقبة تقدم الموقع عن كثب من خلال Google Search
47
7
7
5 7
5
2
10
2
2
10
2
Console، وخاصة تقرير التغطية (Coverage Report) الذي سيظهر الآن عددًا كبيرًا من الصفحات
التي تم فهرستها بنجاح، بالإضافة إلى تقرير Indexing Rate الذي سيظهر زيادة في معدل الفهرسة
هذه العملية، من الإصلاح إلى المراقبة، هي الخطوة الأولى والأساسية والأكثر أهمية في أي .
استراتيجية لتحسين محركات البحث ناجحة لموقع aldeyarksa.tech.
التأثير على المشكلة التقنية SEO الحل المقترح
Disallow: / في
ملف robots.txt
منع محركات البحث من
تصفح أو فهرسة أي جزء من
.الموقع، مما يجعله غير مرئي
استبدال القاعدة ب Allow: / لجميع
المستخدمين (User-agent: *) أو
فقط ل Googlebot.
عدم وجود سطر
Sitemap:
يبطئ من اكتشاف ملف
الهبوط بواسطة محركات
البحث، مما قد يؤخر فهرسة
.المحتوى الجديد
إضافة سطر Sitemap: https://
aldeyarksa.tech/sitemap.xml
في نهاية ملف robots.txt.
حظر الموارد
الأساسية (CSS/JS)
يعيق قدرة Googlebot على
فهم التصميم والهيكل، مما
قد يؤدي إلى فهرسة خاطئة
.أو مظهر سيئ في البحث
التأكد من عدم وجود قواعد Disallow
تمنع تحميل ملفات CSS و JavaScript
.اللازمة لعرض الصفحة
أخطاء نحوية في
ملف robots.txt
يمكن أن تسبب سلوكاً غير
،متوقع في محركات البحث
مثل حظر صفحات غير
.مصرح بها
التحقق من صحة القواعد (حساسية
الأحرف، المسافات، الأشكال النحوية
.الصحيحة) باستخدام أدوات الاختبار
باختصار، إن إصلاح ملف robots.txt ليس مجرد مهمة فنية، بل هو استعادة لحقوق الموقع في
المشاركة في المشهد الرقمي. بدون هذا الإصلاح، فإن كل الجهود الأخرى المتعلقة بالمحتوى
والروابط الخارجية ستكون باطلة. بمجرد حل هذه المشكلة، يصبح الموقع قادرًا على التواصل مع
محركات البحث، ويمكننا بعد ذلك الانتقال إلى مراحل أكثر تطورًا من استراتيجية SEO لبناء
.مصداقيته وجاذبيته وتحقيق الترتيب الأول